{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silviiaaslv/Final-Project-Data-Mining---Kelompok-Erupsi/blob/main/1.%20Crawling%20Data%20Keyword%20Erupsi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crawling Data**"
      ],
      "metadata": {
        "id": "CjuTczNR56pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada bagian pertama ini, dilakukan proses crawling data tweet Twitter yang ditarik dengan bantuan Twitter API. Tweet ditarik berdasarkan pada keyword \"erupsi\", kemudian keyword diperluas menjadi: \"erupsi\", \"gunung meletus\", :\"letus\", \"semburan gunung\", \"semeru\", dan \"semeru erupsi\". Proses penarikan data dilakukan pada tanggal 16 - 17 Desember 2021. \n",
        "\n"
      ],
      "metadata": {
        "id": "5ZTfYYxGKy06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk setiap keyword, dilakukan penarikan data berdasarkan algoritma dibawah ini. Definisikan keyword sebagai search_key dan definisikan nama file sebagai nama_file."
      ],
      "metadata": {
        "id": "JsgggejyW8CX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DK2wAYV2eWi",
        "outputId": "02264a1e-c0eb-4ace-c5ff-c26715c62817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Habis\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "import csv\n",
        "\n",
        "api_key = 'nZAunzCRkIfwY3U3awdLfUzd6'\n",
        "api_key_secret = 'Bm1ojdumaOmPuUwLXjJZieeDSOL6YeBDCVkShE52ZZbSBGVvYM'\n",
        "access_token = '1348194120582381570-5vehUcw0MfjAEwr5ioKI3K7ePF83jK'\n",
        "access_token_secret = 'D4SMA8BNfabRZjdIkMku7tJ8Xuat6Wb3eNBNDcs9XjGgz'\n",
        "\n",
        "tweetsPerQry = 100\n",
        "maxTweets = 300000\n",
        "nama_file = 'gunungmeletus_17122021' #masukkan nama file disini\n",
        "search_key = \"gunung meletus\" #masukkan keyword disini\n",
        "maxId = -1\n",
        "tweetCount = 0\n",
        "\n",
        "authentication = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "authentication.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(authentication, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
        "\n",
        "while tweetCount < maxTweets:\n",
        "    if maxId <= 0 :\n",
        "        newTweets = api.search(q=search_key, count=tweetsPerQry, result_type=\"recent\", tweet_mode = \"extended\")\n",
        "    \n",
        "    newTweets = api.search(q=search_key, count=tweetsPerQry, result_type=\"recent\", tweet_mode = \"extended\", max_id=str(maxId-1))\n",
        "\n",
        "    if not newTweets :\n",
        "        print(\"Tweet Habis\")\n",
        "        break\n",
        "\n",
        "    for tweet in newTweets:\n",
        "        dictTweet = {\n",
        "            \"created_at\" : tweet.created_at,\n",
        "            \"tweet\" : tweet.full_text.encode('utf-8'),\n",
        "            \"favorite_count\" : tweet.favorite_count,\n",
        "            \"retweet_count\" : tweet.retweet_count\n",
        "        }\n",
        "        # print(\"Username {username} : {tweet}\".format(username=dictTweet[\"username\"], tweet=dictTweet[\"tweet\"]))\n",
        "        with open(nama_file+\".csv\", 'a+', newline='') as csv_file:\n",
        "            fieldNames = [\"created_at\", \"tweet\", \"favorite_count\", \"retweet_count\"]\n",
        "            writer = csv.DictWriter(csv_file, fieldnames = fieldNames, delimiter=\";\",)\n",
        "            writer.writerow(dictTweet)\n",
        "\n",
        "    tweetCount += len(newTweets)\t\n",
        "    maxId = newTweets[-1].id"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dari proses crawling tersebut, diperoleh 6 data raw yang disimpan dalam format csv, yaitu:\n",
        "\n",
        "1. erupsi.csv\n",
        "2. erupsi_17122021.csv\n",
        "3. gunungmeletus_17122021.csv\n",
        "4. letus_17122021.csv\n",
        "5. semburangunung_17122021.csv\n",
        "6. semeru_17122021.csv\n",
        "7. semeruerupsi_17122021.csv"
      ],
      "metadata": {
        "id": "yY6mvEswWoPr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8MRDWTNXoQY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1. Crawling Data Keyword Erupsi .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
